<!-- omit in toc -->
# Vaxformer: Immunogenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2

This repository contains pre-trained models, corpora, indices, and code for pre-training, finetuning, retrieving and evaluating for "Vaxformer: Immunogenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2" (Paper in writing)

<!-- omit in toc -->
## Table of Contents
- [ğŸ› ï¸ Setup](#ï¸-setup)
  - [Python packages](#python-packages)
  - [netMHCpan](#netmhcpan)
  - [Alphafold 2](#alphafold-2)
  - [Dataset](#dataset)
- [âŒ¨ï¸ Codebase Structure](#ï¸-codebase-structure)
- [ğŸ¤– Training](#-training)
  - [Prepare the dataset](#prepare-the-dataset)
  - [Training the model](#training-the-model)
- [ğŸ“ Generating sequences](#-generating-sequences)
- [ğŸ“ˆ Evaluation](#-evaluation)
  - [DDGun](#ddgun)
  - [netMHCpan](#netmhcpan-1)
  - [AlphaFold 2](#alphafold-2-1)


## ğŸ› ï¸ Setup
### Python packages
This codebase requires the following dependencies:
```
- biopython
- matplotlib
- numpy
- pandas
- pydantic
- python-dotenv
- PyYAML
- tqdm
- wandb
```

We opted in to using conda as our package manager. The following will install all necessary dependencies for a GPU training:
```
ENV_NAME=vaxformer
conda create -n ${ENV_NAME} python=3.8 -y
conda activate ${ENV_NAME}
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia -y
pip install -r requirements.txt
```


### netMHCpan
> **Note**
> 
> netMHCpan only runs in Linux or Darwin machine

Follow this step to setup netMHCpan:
1. Download https://services.healthtech.dtu.dk/services/NetMHCpan-4.1/
2. Follow their installation instruction outlined in the `netMHCpan-4.1.readme` file


### Alphafold 2


### Dataset
Vaxformer is trained with a dataset of spike Covid proteins from [GI-SAID](https://gisaid.org/register/). You have to have the appropriate GI-SAID credentials to download the dataset.
To obtain comparable data splits, we inquired to [the author of the previous publication (ImmuneConstrainedVAE)](https://github.com/hcgasser/SpikeVAE).

## âŒ¨ï¸ Codebase Structure
```
.
â”œâ”€â”€ configs                                       # Config files
â”‚Â Â  â”œâ”€â”€ test/                                     # Config files for sampling runs
â”‚Â Â  â””â”€â”€ train/                                    # Config files for training runs
â”œâ”€â”€ datasets/                                     # Datasets of sequences and immunogenicity scores
â”œâ”€â”€ scripts                                       # Scripts to start runs
â”‚Â Â  â”œâ”€â”€ netmhcpan/
â”‚   â”‚Â Â  â”œâ”€â”€ netmhcpan_allele_scores.sh            # Script to run netMHCpan scoring for peptide files
â”‚   â”‚Â Â  â”œâ”€â”€ generate_peptides_from_sequences.py   # Script to generate peptides from sequences
â”‚Â Â  â”œâ”€â”€ slurm/                                    # Slurm scripts for training and sampling runs
â”‚Â Â  â”œâ”€â”€ sample.py                                 # Script to run sampling with a model of choice
â”‚Â Â  â””â”€â”€ train.py                                  # Script to run training with a model configuration of choice
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ dataset
â”‚   â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚   â”‚Â Â  â”œâ”€â”€ dataset.py                            # Dataset class to prepare and iterate through the dataset
â”‚   â”‚Â Â  â””â”€â”€ tokenizer.py                          # Tokenizer class to preprocess the input sequence
â”‚   â”œâ”€â”€ models 
â”‚   â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚   â”‚Â Â  â”œâ”€â”€ baseline.py                           # Naive Bayes baseline model
â”‚   â”‚Â Â  â”œâ”€â”€ lstm.py                               # Conditional LSTM model
â”‚   â”‚Â Â  â”œâ”€â”€ vae.py                                # Conditional VAE model
â”‚   â”‚Â Â  â””â”€â”€ vaxformer.py                          # The proposed Vaxformer model
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ common_utils.py                       # Utility functions to prepare trainings
â”‚   â”‚   â””â”€â”€ model_utils.py                        # Utility functions for modelling purposes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ configs.py                                # Pydantic configs validator
â”‚   â”œâ”€â”€ constants.py                              # Constants for the training
â”‚   â””â”€â”€ trainer.py                                # Trainer class to handle training operations
â”œâ”€â”€ requirements.txt                              # Necessary Python Packages
â”œâ”€â”€ README.md                                     # You are here
```

## ğŸ¤– Training

### Prepare the dataset

```

```

### Training the model
Once the sequences and immunogenicity scores datasets are obtained, we can run a training process.
```
python scripts/train.py \
--config_filepath=PATH/TO/TRAIN_CONFIG_FILE
```
Selections of train config files can be found in the [`configs/train/`](https://github.com/aryopg/vaxformer/tree/main/configs/train) folder.

Both LSTM and Vaxformer can be trained with `-small`, `-base`, or `-large` setting. They differ in terms of the number of hidden layers and their sizes.

## ğŸ“ Generating sequences

Before any evaluation steps, we need to first generate the sequences with a pretrained model of choice

```
python scripts/sample.py \
--config_filepath=PATH/TO/TEST_CONFIG_FILE
--num_sequences=2000
```

Examples of test config files can be found in the [`configs/test/`](https://github.com/aryopg/vaxformer/tree/main/configs/test) folder.

## ğŸ“ˆ Evaluation


### DDGun


### netMHCpan
To evaluate the generated the netMHCpan
```

```

### AlphaFold 2

